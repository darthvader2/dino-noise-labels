{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":106,"metadata":{"id":"ryDEXt0HEbDU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cdc681a-a144-4e6a-b6f3-85f723f1548f","executionInfo":{"status":"ok","timestamp":1704484517794,"user_tz":-60,"elapsed":1447,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as pth_transforms\n","from sklearn.neighbors import KNeighborsClassifier\n","from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n","import torch.utils.data as Data\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","# Append the directory to your python path using sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/')\n","\n"]},{"cell_type":"code","source":["def get_instance_noisy_label(n, dataset, labels, num_classes, feature_size, norm_std, seed):\n","    # n -> noise_rate\n","    # dataset -> mnist, cifar10 # not train_loader\n","    # labels -> labels (targets)\n","    # label_num -> class number\n","    # feature_size -> the size of input images (e.g. 28*28)\n","    # norm_std -> default 0.1\n","    # seed -> random_seed\n","    print(\"building dataset...\")\n","    label_num = num_classes\n","    np.random.seed(int(seed))\n","    torch.manual_seed(int(seed))\n","    torch.cuda.manual_seed(int(seed))\n","\n","    P = []\n","    flip_distribution = stats.truncnorm((0 - n) / norm_std, (1 - n) / norm_std, loc=n, scale=norm_std)\n","    flip_rate = flip_distribution.rvs(labels.shape[0])\n","\n","    if isinstance(labels, list):\n","        labels = torch.FloatTensor(labels)\n","    labels = labels.cuda()\n","\n","    W = np.random.randn(label_num, feature_size, label_num)\n","\n","\n","    W = torch.FloatTensor(W).cuda()\n","    for i, (x, y) in enumerate(dataset):\n","        # 1*m *  m*10 = 1*10\n","        x = x.cuda()\n","        A = x.view(1, -1).mm(W[y]).squeeze(0)\n","        A[y] = -inf\n","        A = flip_rate[i] * F.softmax(A, dim=0)\n","        A[y] += 1 - flip_rate[i]\n","        P.append(A)\n","    P = torch.stack(P, 0).cpu().numpy()\n","    l = [i for i in range(label_num)]\n","    new_label = [np.random.choice(l, p=P[i]) for i in range(labels.shape[0])]\n","    record = [[0 for _ in range(label_num)] for i in range(label_num)]\n","\n","    for a, b in zip(labels, new_label):\n","        a, b = int(a), int(b)\n","        record[a][b] += 1\n","\n","\n","    pidx = np.random.choice(range(P.shape[0]), 1000)\n","    cnt = 0\n","    for i in range(1000):\n","        if labels[pidx[i]] == 0:\n","            a = P[pidx[i], :]\n","            cnt += 1\n","        if cnt >= 10:\n","            break\n","    return np.array(new_label)\n","def transform_target(label):\n","    label = np.array(label)\n","    target = torch.from_numpy(label).long()\n","    return target\n","\n","def data_split(data, targets, split_percentage, seed=1):\n","\n","    num_samples = int(targets.shape[0])\n","    np.random.seed(int(seed))\n","    train_set_index = np.random.choice(num_samples, int(num_samples*split_percentage), replace=False)\n","    index = np.arange(data.shape[0])\n","    val_set_index = np.delete(index, train_set_index)\n","    train_set, val_set = data[train_set_index, :], data[val_set_index, :]\n","    train_labels, val_labels = targets[train_set_index], targets[val_set_index]\n","\n","    return train_set, val_set, train_labels, val_labels\n","\n","\n","class cifar10_dataset(Data.Dataset):\n","    def __init__(self, train=True, transform=None, target_transform=None, noise_rate=0.2, split_percentage=0.9, seed=1, num_classes=10, feature_size=3*32*32, norm_std=0.1):\n","\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.train = train\n","\n","        original_images = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/train_images.npy')\n","        original_labels = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/train_images.npy')\n","        data = torch.from_numpy(original_images).float()\n","        targets = torch.from_numpy(original_labels)\n","\n","        dataset = zip(data, targets)\n","        new_labels = get_instance_noisy_label(noise_rate, dataset, targets, num_classes, feature_size, norm_std, seed)\n","\n","\n","        self.train_data, self.val_data, self.train_labels, self.val_labels = data_split(original_images, new_labels, split_percentage,seed)\n","        if self.train:\n","            self.train_data = self.train_data.reshape((-1,3,32,32))\n","            self.train_data = self.train_data.transpose((0, 2, 3, 1))\n","            print(self.train_data.shape)\n","\n","        else:\n","            self.val_data = self.val_data.reshape((-1, 3,32,32))\n","            self.val_data = self.val_data.transpose((0, 2, 3, 1))\n","\n","    def __getitem__(self, index):\n","\n","        if self.train:\n","            img, label = self.train_data[index], self.train_labels[index]\n","\n","        else:\n","            img, label = self.val_data[index], self.val_labels[index]\n","\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            label = self.target_transform(label)\n","\n","        return img, label\n","    def __len__(self):\n","\n","        if self.train:\n","            return len(self.train_data)\n","\n","        else:\n","            return len(self.val_data)"],"metadata":{"id":"F2puDt2-J4gF","executionInfo":{"status":"ok","timestamp":1704484522179,"user_tz":-60,"elapsed":6,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["num_classes = 10\n","feature_size = 3 * 32 * 32\n","n_epoch_1, n_epoch_2, n_epoch_3 = 5, 50, 50\n","dim = 512\n","basis = 20\n","iteration_nmf = 10\n","train_dataset = cifar10_dataset(True,\n","                                    transform = transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)),\n","                                    ]),\n","                                    target_transform=transform_target)\n"],"metadata":{"id":"Mozh10lmElIc","colab":{"base_uri":"https://localhost:8080/","height":462},"executionInfo":{"status":"error","timestamp":1704484546756,"user_tz":-60,"elapsed":1506,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}},"outputId":"d7805191-b149-4a04-d784-ddbb7165b88e"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["building dataset...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-107-1c252bcca995>:30: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n","  A = x.view(1, -1).mm(W[y]).squeeze(0)\n"]},{"output_type":"error","ename":"IndexError","evalue":"The shape of the mask [3072] at index 0 does not match the shape of the indexed tensor [10, 3072, 10] at index 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-109-de5fa762c157>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbasis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0miteration_nmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_dataset = cifar10_dataset(True,\n\u001b[0m\u001b[1;32m      8\u001b[0m                                     transform = transforms.Compose([\n\u001b[1;32m      9\u001b[0m                                     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-107-1c252bcca995>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, transform, target_transform, noise_rate, split_percentage, seed, num_classes, feature_size, norm_std)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance_noisy_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-107-1c252bcca995>\u001b[0m in \u001b[0;36mget_instance_noisy_label\u001b[0;34m(n, dataset, labels, num_classes, feature_size, norm_std, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# 1*m *  m*10 = 1*10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflip_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: The shape of the mask [3072] at index 0 does not match the shape of the indexed tensor [10, 3072, 10] at index 0"]}]},{"cell_type":"code","source":["torch.hub.list('facebookresearch/dino:main')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJeGVnh3tc8s","outputId":"2de93fba-5cdc-461f-c5a9-ecc7e5e2ad80","executionInfo":{"status":"ok","timestamp":1704315841940,"user_tz":-60,"elapsed":328,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"]},{"output_type":"execute_result","data":{"text/plain":["['dino_resnet50',\n"," 'dino_vitb16',\n"," 'dino_vitb8',\n"," 'dino_vits16',\n"," 'dino_vits8',\n"," 'dino_xcit_medium_24_p16',\n"," 'dino_xcit_medium_24_p8',\n"," 'dino_xcit_small_12_p16',\n"," 'dino_xcit_small_12_p8',\n"," 'resnet50']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["torch.hub.help('facebookresearch/dino:main', 'dino_vitb16')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"t9M1zyeVtzgy","outputId":"388d473b-46f1-4553-ff48-ca2f1ae43c11","executionInfo":{"status":"ok","timestamp":1704315844317,"user_tz":-60,"elapsed":305,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n    ViT-Base/16x16 pre-trained with DINO.\\n    Achieves 76.1% top-1 accuracy on ImageNet with k-NN classification.\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16', pretrained=True)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    model.cuda()\n","model.eval()"],"metadata":{"id":"lpT2Zb8DErp0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4072435b-69a9-4a9b-9d78-d3140b3d8f12","executionInfo":{"status":"ok","timestamp":1704318403695,"user_tz":-60,"elapsed":12135,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n","Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_vitbase16_pretrain.pth\n","100%|██████████| 327M/327M [00:07<00:00, 48.6MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (blocks): ModuleList(\n","    (0-11): 12 x Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (head): Identity()\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from torch import nn\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","class KNN():\n","    def __init__(self, model, k, device):\n","        super(KNN, self).__init__()\n","        self.k = k\n","        self.device = device\n","        self.model = model.to(device)\n","        self.model.eval()\n","\n","    def extract_features(self, loader):\n","        \"\"\"\n","        Infer/Extract features from a trained model\n","        Args:\n","            loader: train or test loader\n","        Returns: 3 tensors of all:  input_images, features , labels\n","        \"\"\"\n","        x_lst = []\n","        features = []\n","        label_lst = []\n","\n","        with torch.no_grad():\n","            for input_tensor, label in loader:\n","                h = self.model(input_tensor.to(self.device))\n","                features.append(h)\n","                x_lst.append(input_tensor)\n","                label_lst.append(label)\n","\n","            x_total = torch.stack(x_lst)\n","            h_total = torch.stack(features)\n","            label_total = torch.stack(label_lst)\n","\n","            return x_total, h_total, label_total\n","\n","    def knn(self, features, labels, k=1):\n","        \"\"\"\n","        Evaluating knn accuracy in feature space.\n","        Calculates only top-1 accuracy (returns 0 for top-5)\n","        Args:\n","            features: [... , dataset_size, feat_dim]\n","            labels: [... , dataset_size]\n","            k: nearest neighbours\n","        Returns: train accuracy, or train and test acc\n","        \"\"\"\n","        feature_dim = features.shape[-1]\n","        with torch.no_grad():\n","            features_np = features.cpu().view(-1, feature_dim).numpy()\n","            labels_np = labels.cpu().view(-1).numpy()\n","            # fit\n","            self.cls = KNeighborsClassifier(k, metric=\"cosine\").fit(features_np, labels_np)\n","            acc = self.eval(features, labels)\n","\n","        return acc\n","\n","    def eval(self, features, labels):\n","      feature_dim = features.shape[-1]\n","      features = features.cpu().view(-1, feature_dim).numpy()\n","      labels = labels.cpu().view(-1).numpy()\n","      acc = 100 * np.mean(cross_val_score(self.cls, features, labels))\n","      predicted_labels = self.cls.predict(features)\n","      classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","      num_classes=10\n","      noise_conf_matrix = confusion_matrix(labels, predicted_labels)\n","      noise_transitions = noise_conf_matrix / noise_conf_matrix.sum(axis=1, keepdims=True)\n","      plt.figure(figsize=(12, 4))\n","      plt.subplot(1, 2, 2)\n","      sns.heatmap(noise_transitions, annot=True, cmap=\"Reds\", fmt=\".2f\", cbar=False,\n","                  xticklabels=[f'{classes[i]}' for i in range(num_classes)],\n","                  yticklabels=[f'{classes[i]}' for i in range(num_classes)])\n","      plt.title('Noise Label Transition Matrix')\n","      return acc\n","\n","    def _find_best_indices(self, h_query, h_ref):\n","        h_query = h_query / h_query.norm(dim=1).view(-1, 1)\n","        h_ref = h_ref / h_ref.norm(dim=1).view(-1, 1)\n","        scores = torch.matmul(h_query, h_ref.t())  # [query_bs, ref_bs]\n","        score, indices = scores.topk(1, dim=1)  # select top k best\n","        return score, indices\n","\n","    def fit(self, test_loader=None):\n","        with torch.no_grad():\n","          x_test, h_test, l_test = self.extract_features(test_loader)\n","          test_acc = self.knn(h_test, l_test, k=self.k)\n","          return(test_acc)"],"metadata":{"id":"Iuwd6f_ODjof","executionInfo":{"status":"ok","timestamp":1704482977380,"user_tz":-60,"elapsed":5,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["ssl_evaluator = KNN(model=model, k=1, device='cuda')\n","ssl_evaluator.fit(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":946},"id":"Ipq3ftyCDkf7","executionInfo":{"status":"error","timestamp":1704484052808,"user_tz":-60,"elapsed":567,"user":{"displayName":"konda varshith","userId":"04194470581004097793"}},"outputId":"318e6554-28b9-4182-8eb2-52f904fbcd69"},"execution_count":89,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 3)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-b3d5e7422901>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssl_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssl_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-81-9b41ee73cc67>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, test_loader)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m           \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m           \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-81-9b41ee73cc67>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mx_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py\u001b[0m in \u001b[0;36mprepare_tokens\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# patch linear embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","def plot_result(query_image, retrieved_indices, dataset):\n","    # Plot the query image\n","    plt.subplot(2, 3, 1)\n","    plt.imshow(query_image)\n","    plt.title(\"Query Image\")\n","\n","    # Plot the top 5 retrieved images\n","    for i, idx in enumerate(retrieved_indices):\n","        image_path = dataset.image_paths[idx]\n","        image = Image.open(image_path)\n","\n","        plt.subplot(2, 3, i + 2)\n","        plt.imshow(image)\n","        plt.title(f\"Retrieved Image {i+1}\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Example usage:\n","# Assuming you have an instance of ImageRetrievalKNN named 'image_retrieval_knn'\n","query_image, query_label = test_dataset[0]  # Replace with your query image\n","retrieved_indices = image_retrieval_knn.image_retrieval(query_image)\n","\n","# Plot the result\n","plot_result(query_image, retrieved_indices, test_dataset)\n"],"metadata":{"id":"T1mtoxYbSq0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from tqdm.notebook import tqdm\n","\n","features = []\n","labels = []  # Create a list to store labels\n","for samples, index in tqdm(data_loader_train, desc='Batches'):\n","    samples = samples.cuda(non_blocking=True)\n","    #index = index.cuda(non_blocking=True)  # We don't need the index on GPU\n","    with torch.no_grad():\n","        #feature_batch = model(samples).clone()\n","        # Let's not keep the features around on the GPU, also cloning keeps\n","        # reference to the computational graph which could cause memory issues\n","        # outside of no_grad().\n","        # detach() decouples the computation from the graph which created it.\n","        # Since we're in a no_grad() context, that probably doesn't matter since\n","        # the results are likely not chached.\n","        feature_batch = model(samples)\n","    features.append(feature_batch)\n","    labels.append(index)\n","\n","# Combine the feature batches into a single tensor\n","features_tensor = torch.cat(features, dim=0)\n","labels_tensor = torch.cat(labels, dim=0)\n","\n","print(f\"Storing features into tensor of shape {features_tensor.shape}\")\n","print(f\"Storing labels into tensor of shape {labels_tensor.shape}\")\n"],"metadata":{"id":"dDekvjS7E1Yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.manifold import TSNE\n","\n","features_array = features_tensor.numpy()\n","labels_array = labels_tensor.cpu().numpy()\n","\n","tsne = TSNE(n_components=3)\n","reduced_features = tsne.fit_transform(features_array)"],"metadata":{"id":"x916ZPh5E2J_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_array"],"metadata":{"id":"CtbLuW8P_Zzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(16, 12))\n","plt.scatter(reduced_features[:, 0], reduced_features[:, 1], s=2, alpha=.5, c=labels_array, cmap='tab10')\n","plt.title(\"t-SNE Clustering of Training Features\")\n","#plt.colorbar()\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nxFpZAC8FFdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","knn_classifier = KNeighborsClassifier(n_neighbors=5)\n","knn_classifier.fit(reduced_features, labels_array)"],"metadata":{"id":"B56cD1cCyEVK"},"execution_count":null,"outputs":[]}]}