{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd1odKauRAgE",
        "outputId": "48366203-07f4-4485-df89-b2bd234b56dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "bVa-2cfFTfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVc62RPHdJbN",
        "outputId": "fa21cbf5-f9af-4749-e4f2-b6b4e5444c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import models as torchvision_models\n",
        "\n",
        "import torch.utils.data as Data\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "from vision_transformer import DINOHead\n",
        "from scipy import stats\n",
        "from math import inf"
      ],
      "metadata": {
        "id": "L_aJF9T3kQ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data generator\n"
      ],
      "metadata": {
        "id": "6U9tT0gztMHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def transform_target(label):\n",
        "    label = np.array(label)\n",
        "    target = torch.from_numpy(label).long()\n",
        "    return target\n",
        "\n",
        "def data_split(data, targets, split_percentage, seed=1):\n",
        "\n",
        "    num_samples = int(targets.shape[0])\n",
        "    np.random.seed(int(seed))\n",
        "    train_set_index = np.random.choice(num_samples, int(num_samples*split_percentage), replace=False)\n",
        "    index = np.arange(data.shape[0])\n",
        "    val_set_index = np.delete(index, train_set_index)\n",
        "    train_set, val_set = data[train_set_index, :], data[val_set_index, :]\n",
        "    train_labels, val_labels = targets[train_set_index], targets[val_set_index]\n",
        "    return train_set, val_set, train_labels, val_labels\n",
        "\n",
        "class cifar10_dataset(Data.Dataset):\n",
        "    def __init__(self, train=True, transform=None, target_transform=None, noise_rate=0.8, split_percentage=0.9, seed=1, num_classes=10, feature_size=3*32*32, norm_std=0.1):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train\n",
        "\n",
        "        original_images = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/train_images.npy')\n",
        "        original_labels = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/train_labels.npy')\n",
        "        data = torch.from_numpy(original_images).float()\n",
        "        targets = torch.from_numpy(original_labels)\n",
        "\n",
        "        dataset = zip(data, targets)\n",
        "        new_labels = get_instance_noisy_label(noise_rate, dataset, targets, num_classes, feature_size, norm_std, seed)\n",
        "\n",
        "\n",
        "        self.train_data, self.train_labels= original_images, new_labels\n",
        "        print(self.train_data.shape)\n",
        "        self.train_data = self.train_data.reshape((-1,3,32,32))\n",
        "        self.train_data = self.train_data.transpose((0, 2, 3, 1))\n",
        "        print(self.train_data.shape)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.train:\n",
        "            img, label = self.train_data[index], self.train_labels[index]\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label\n",
        "    def __len__(self):\n",
        "\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "class cifar10_test_dataset(Data.Dataset):\n",
        "    def __init__(self, transform=None, target_transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.test_data = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/test_images.npy')\n",
        "        self.test_labels = np.load('/content/drive/MyDrive/Colab Notebooks/data/cifar10/test_labels.npy')\n",
        "        self.test_data = self.test_data.reshape((10000,3,32,32))\n",
        "        self.test_data = self.test_data.transpose((0, 2, 3, 1))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img, label = self.test_data[index], self.test_labels[index]\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_data)"
      ],
      "metadata": {
        "id": "TldmrDcgtLTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_instance_noisy_label(n, dataset, labels, num_classes, feature_size, norm_std, seed):\n",
        "    # n -> noise_rate\n",
        "    # dataset -> mnist, cifar10 # not train_loader\n",
        "    # labels -> labels (targets)\n",
        "    # label_num -> class number\n",
        "    # feature_size -> the size of input images (e.g. 28*28)\n",
        "    # norm_std -> default 0.1\n",
        "    # seed -> random_seed\n",
        "    print(\"building dataset...\")\n",
        "    label_num = num_classes\n",
        "    np.random.seed(int(seed))\n",
        "    torch.manual_seed(int(seed))\n",
        "    torch.cuda.manual_seed(int(seed))\n",
        "\n",
        "    P = []\n",
        "    flip_distribution = stats.truncnorm((0 - n) / norm_std, (1 - n) / norm_std, loc=n, scale=norm_std)\n",
        "    flip_rate = flip_distribution.rvs(labels.shape[0])\n",
        "\n",
        "    if isinstance(labels, list):\n",
        "        labels = torch.FloatTensor(labels)\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    W = np.random.randn(label_num, feature_size, label_num)\n",
        "\n",
        "\n",
        "    W = torch.FloatTensor(W).cuda()\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        # 1*m *  m*10 = 1*10\n",
        "        x = x.cuda()\n",
        "        A = x.view(1, -1).mm(W[y]).squeeze(0)\n",
        "        A[y] = -inf\n",
        "        A = flip_rate[i] * F.softmax(A, dim=0)\n",
        "        A[y] += 1 - flip_rate[i]\n",
        "        P.append(A)\n",
        "    P = torch.stack(P, 0).cpu().numpy()\n",
        "    l = [i for i in range(label_num)]\n",
        "    new_label = [np.random.choice(l, p=P[i]) for i in range(labels.shape[0])]\n",
        "    record = [[0 for _ in range(label_num)] for i in range(label_num)]\n",
        "\n",
        "    for a, b in zip(labels, new_label):\n",
        "        a, b = int(a), int(b)\n",
        "        record[a][b] += 1\n",
        "\n",
        "\n",
        "    pidx = np.random.choice(range(P.shape[0]), 1000)\n",
        "    cnt = 0\n",
        "    for i in range(1000):\n",
        "        if labels[pidx[i]] == 0:\n",
        "            a = P[pidx[i], :]\n",
        "            cnt += 1\n",
        "        if cnt >= 10:\n",
        "            break\n",
        "    return np.array(new_label)"
      ],
      "metadata": {
        "id": "VILlSmIftlbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "feature_size = 3 * 32 * 32\n",
        "n_epoch_1, n_epoch_2, n_epoch_3 = 5, 50, 50\n",
        "dim = 512\n",
        "basis = 20\n",
        "iteration_nmf = 10\n",
        "train_dataset = cifar10_dataset(True,target_transform=transform_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8bd6oWtnzl",
        "outputId": "c7db1b1e-54a6-4c41-eff0-4161a032506a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building dataset...\n",
            "(50000, 3072)\n",
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q00MJIhztcQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dino model\n"
      ],
      "metadata": {
        "id": "w4I9hfiatu3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,\n",
        "                    optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,\n",
        "                    fp16_scaler, args):\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)\n",
        "    for it, (images, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):\n",
        "        # update weight decay and learning rate according to their schedule\n",
        "        it = len(data_loader) * epoch + it  # global training iteration\n",
        "        for i, param_group in enumerate(optimizer.param_groups):\n",
        "            param_group[\"lr\"] = lr_schedule[it]\n",
        "            if i == 0:  # only the first group is regularized\n",
        "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
        "\n",
        "        # move images to gpu\n",
        "        images = [im.cuda(non_blocking=True) for im in images]\n",
        "        # teacher and student forward passes + compute dino loss\n",
        "        with torch.cuda.amp.autocast(fp16_scaler is not None):\n",
        "            teacher_output = teacher(images[:2])  # only the 2 global views pass through the teacher\n",
        "            student_output = student(images)\n",
        "            loss = dino_loss(student_output, teacher_output, epoch)\n",
        "\n",
        "        if not math.isfinite(loss.item()):\n",
        "            print(\"Loss is {}, stopping training\".format(loss.item()), force=True)\n",
        "            sys.exit(1)\n",
        "\n",
        "        # student update\n",
        "        optimizer.zero_grad()\n",
        "        param_norms = None\n",
        "        if fp16_scaler is None:\n",
        "            loss.backward()\n",
        "            if args.clip_grad:\n",
        "                param_norms = utils.clip_gradients(student, args.clip_grad)\n",
        "            utils.cancel_gradients_last_layer(epoch, student,\n",
        "                                              args.freeze_last_layer)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            fp16_scaler.scale(loss).backward()\n",
        "            if args.clip_grad:\n",
        "                fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
        "                param_norms = utils.clip_gradients(student, args.clip_grad)\n",
        "            utils.cancel_gradients_last_layer(epoch, student,\n",
        "                                              args.freeze_last_layer)\n",
        "            fp16_scaler.step(optimizer)\n",
        "            fp16_scaler.update()\n",
        "\n",
        "        # EMA update for the teacher\n",
        "        with torch.no_grad():\n",
        "            m = momentum_schedule[it]  # momentum parameter\n",
        "            for param_q, param_k in zip(student.parameters(), teacher_without_ddp.parameters()):\n",
        "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
        "\n",
        "        # logging\n",
        "        torch.cuda.synchronize()\n",
        "        metric_logger.update(loss=loss.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "        metric_logger.update(wd=optimizer.param_groups[0][\"weight_decay\"])\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
      ],
      "metadata": {
        "id": "tpzdDebvGpCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentationDINO(object):\n",
        "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
        "        flip_and_color_jitter = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply(\n",
        "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
        "                p=0.8\n",
        "            ),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "        ])\n",
        "        normalize = transforms.Compose([\n",
        "            transforms.Resize(256, interpolation=3),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        # first global crop\n",
        "        self.global_transfo1 = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            utils.GaussianBlur(1.0),\n",
        "            normalize,\n",
        "        ])\n",
        "        # second global crop\n",
        "        self.global_transfo2 = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            utils.GaussianBlur(0.1),\n",
        "            utils.Solarization(0.2),\n",
        "            normalize,\n",
        "        ])\n",
        "        # transformation for the local small crops\n",
        "        self.local_crops_number = local_crops_number\n",
        "        self.local_transfo = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            utils.GaussianBlur(p=0.5),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    def __call__(self, image):\n",
        "        crops = []\n",
        "        crops.append(self.global_transfo1(image))\n",
        "        crops.append(self.global_transfo2(image))\n",
        "        for _ in range(self.local_crops_number):\n",
        "            crops.append(self.local_transfo(image))\n",
        "        return crops"
      ],
      "metadata": {
        "id": "EbOOmcaHeI6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOLoss(nn.Module):\n",
        "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
        "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.ncrops = ncrops\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "        # we apply a warm up for the teacher temperature because\n",
        "        # a too high temperature makes the training instable at the beginning\n",
        "        self.teacher_temp_schedule = np.concatenate((\n",
        "            np.linspace(warmup_teacher_temp,\n",
        "                        teacher_temp, warmup_teacher_temp_epochs),\n",
        "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
        "        ))\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
        "        \"\"\"\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = student_out.chunk(self.ncrops)\n",
        "\n",
        "        # teacher centering and sharpening\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach().chunk(2)\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                if v == iq:\n",
        "                    # we skip cases where student and teacher operate on the same view\n",
        "                    continue\n",
        "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update center used for teacher output.\n",
        "        \"\"\"\n",
        "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
        "        batch_center = batch_center / len(teacher_output)\n",
        "\n",
        "        # ema update\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
      ],
      "metadata": {
        "id": "SDafGV3xDEOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dino(args):\n",
        "    torch.cuda.is_available()\n",
        "    args.gpu=0\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    utils.fix_random_seeds(args.seed)\n",
        "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
        "    print(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # ============ preparing data ... ============\n",
        "    transform = DataAugmentationDINO(\n",
        "        args.global_crops_scale,\n",
        "        args.local_crops_scale,\n",
        "        args.local_crops_number,\n",
        "    )\n",
        "    dataset = cifar10_dataset(True,transform=transform,target_transform=transform_target)\n",
        "    sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=len(dataset))\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=args.batch_size_per_gpu,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    print(f\"Data loaded: there are {len(dataset)} images.\")\n",
        "\n",
        "    # ============ building student and teacher networks ... ============\n",
        "    # we changed the name DeiT-S for ViT-S to avoid confusions\n",
        "    args.arch = args.arch.replace(\"deit\", \"vit\")\n",
        "    # if the network is a Vision Transformer (i.e. vit_tiny, vit_small, vit_base)\n",
        "    if args.arch in vits.__dict__.keys():\n",
        "        student = vits.__dict__[args.arch](\n",
        "            patch_size=args.patch_size,\n",
        "            drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
        "        )\n",
        "        teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
        "        embed_dim = student.embed_dim\n",
        "    # if the network is a XCiT\n",
        "    elif args.arch in torch.hub.list(\"facebookresearch/xcit:main\"):\n",
        "        student = torch.hub.load('facebookresearch/xcit:main', args.arch,\n",
        "                                 pretrained=False, drop_path_rate=args.drop_path_rate)\n",
        "        teacher = torch.hub.load('facebookresearch/xcit:main', args.arch, pretrained=False)\n",
        "        embed_dim = student.embed_dim\n",
        "    # otherwise, we check if the architecture is in torchvision models\n",
        "    elif args.arch in torchvision_models.__dict__.keys():\n",
        "        student = torchvision_models.__dict__[args.arch]()\n",
        "        teacher = torchvision_models.__dict__[args.arch]()\n",
        "        embed_dim = student.fc.weight.shape[1]\n",
        "    else:\n",
        "        print(f\"Unknow architecture: {args.arch}\")\n",
        "\n",
        "    # multi-crop wrapper handles forward with inputs of different resolutions\n",
        "    student = utils.MultiCropWrapper(student, DINOHead(\n",
        "        embed_dim,\n",
        "        args.out_dim,\n",
        "        use_bn=args.use_bn_in_head,\n",
        "        norm_last_layer=args.norm_last_layer,\n",
        "    ))\n",
        "    teacher = utils.MultiCropWrapper(\n",
        "        teacher,\n",
        "        DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
        "    )\n",
        "    # move networks to gpu\n",
        "    student, teacher = student.cuda(), teacher.cuda()\n",
        "    # synchronize batch norms (if any)\n",
        "    if utils.has_batchnorms(student):\n",
        "        student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
        "        teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
        "\n",
        "        # we need DDP wrapper to have synchro batch norms working...\n",
        "        teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
        "        teacher_without_ddp = teacher.module\n",
        "    else:\n",
        "        # teacher_without_ddp and teacher are the same thing\n",
        "        teacher_without_ddp = teacher\n",
        "    # teacher and student start with the same weights\n",
        "    teacher_without_ddp.load_state_dict(student.state_dict())\n",
        "    # there is no backpropagation through the teacher, so no need for gradients\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f\"Student and Teacher are built: they are both {args.arch} network.\")\n",
        "\n",
        "    # ============ preparing loss ... ============\n",
        "    dino_loss = DINOLoss(\n",
        "        args.out_dim,\n",
        "        args.local_crops_number + 2,  # total number of crops = 2 global crops + local_crops_number\n",
        "        args.warmup_teacher_temp,\n",
        "        args.teacher_temp,\n",
        "        args.warmup_teacher_temp_epochs,\n",
        "        args.epochs,\n",
        "    ).cuda()\n",
        "\n",
        "    # ============ preparing optimizer ... ============\n",
        "    params_groups = utils.get_params_groups(student)\n",
        "    if args.optimizer == \"adamw\":\n",
        "        optimizer = torch.optim.AdamW(params_groups)  # to use with ViTs\n",
        "    elif args.optimizer == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(params_groups, lr=0, momentum=0.9)  # lr is set by scheduler\n",
        "    elif args.optimizer == \"lars\":\n",
        "        optimizer = utils.LARS(params_groups)  # to use with convnet and large batches\n",
        "    # for mixed precision training\n",
        "    fp16_scaler = None\n",
        "    if args.use_fp16:\n",
        "        fp16_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # ============ init schedulers ... ============\n",
        "    lr_schedule = utils.cosine_scheduler(\n",
        "        args.lr * (args.batch_size_per_gpu * utils.get_world_size()) / 256.,  # linear scaling rule\n",
        "        args.min_lr,\n",
        "        args.epochs, len(data_loader),\n",
        "        warmup_epochs=args.warmup_epochs,\n",
        "    )\n",
        "    wd_schedule = utils.cosine_scheduler(\n",
        "        args.weight_decay,\n",
        "        args.weight_decay_end,\n",
        "        args.epochs, len(data_loader),\n",
        "    )\n",
        "    # momentum parameter is increased to 1. during training with a cosine schedule\n",
        "    momentum_schedule = utils.cosine_scheduler(args.momentum_teacher, 1,\n",
        "                                               args.epochs, len(data_loader))\n",
        "    print(f\"Loss, optimizer and schedulers ready.\")\n",
        "\n",
        "    # ============ optionally resume training ... ============\n",
        "    to_restore = {\"epoch\": 0}\n",
        "    utils.restart_from_checkpoint(\n",
        "        os.path.join(args.output_dir, \"checkpoint.pth\"),\n",
        "        run_variables=to_restore,\n",
        "        student=student,\n",
        "        teacher=teacher,\n",
        "        optimizer=optimizer,\n",
        "        fp16_scaler=fp16_scaler,\n",
        "        dino_loss=dino_loss,\n",
        "    )\n",
        "    start_epoch = to_restore[\"epoch\"]\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"Starting DINO training !\")\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        # ============ training one epoch of DINO ... ============\n",
        "        train_stats = train_one_epoch(student, teacher, teacher_without_ddp, dino_loss,\n",
        "            data_loader, optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
        "            epoch, fp16_scaler, args)\n",
        "\n",
        "        # ============ writing logs ... ============\n",
        "        save_dict = {\n",
        "            'student': student.state_dict(),\n",
        "            'teacher': teacher.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'args': args,\n",
        "            'dino_loss': dino_loss.state_dict(),\n",
        "        }\n",
        "        if fp16_scaler is not None:\n",
        "            save_dict['fp16_scaler'] = fp16_scaler.state_dict()\n",
        "        utils.save_on_master(save_dict, os.path.join(args.output_dir, 'checkpoint.pth'))\n",
        "        if args.saveckp_freq and epoch % args.saveckp_freq == 0:\n",
        "            utils.save_on_master(save_dict, os.path.join(args.output_dir, f'checkpoint{epoch:04}.pth'))\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     'epoch': epoch}\n",
        "        if utils.is_main_process():\n",
        "            with (Path(args.output_dir) / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "metadata": {
        "id": "ldKFHsCgT6rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision_archs = sorted(name for name in torchvision_models.__dict__\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "    and callable(torchvision_models.__dict__[name]))"
      ],
      "metadata": {
        "id": "LgkzWpBHc3tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('DINO', add_help=False)\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--arch', default='vit_tiny', type=str,\n",
        "        choices=['vit_tiny', 'vit_small', 'vit_base', 'xcit', 'deit_tiny', 'deit_small'] \\\n",
        "                + torchvision_archs + torch.hub.list(\"facebookresearch/xcit:main\"),\n",
        "        help=\"\"\"Name of architecture to train. For quick experiments with ViTs,\n",
        "        we recommend using vit_tiny or vit_small.\"\"\")\n",
        "    parser.add_argument('--patch_size', default=16, type=int, help=\"\"\"Size in pixels\n",
        "        of input square patches - default 16 (for 16x16 patches). Using smaller\n",
        "        values leads to better performance but requires more memory. Applies only\n",
        "        for ViTs (vit_tiny, vit_small and vit_base). If <16, we recommend disabling\n",
        "        mixed precision training (--use_fp16 false) to avoid unstabilities.\"\"\")\n",
        "    parser.add_argument('--out_dim', default=65536, type=int, help=\"\"\"Dimensionality of\n",
        "        the DINO head output. For complex and large datasets large values (like 65k) work well.\"\"\")\n",
        "    parser.add_argument('--norm_last_layer', default=True, type=utils.bool_flag,\n",
        "        help=\"\"\"Whether or not to weight normalize the last layer of the DINO head.\n",
        "        Not normalizing leads to better performance but can make the training unstable.\n",
        "        In our experiments, we typically set this paramater to False with vit_small and True with vit_base.\"\"\")\n",
        "    parser.add_argument('--momentum_teacher', default=0.996, type=float, help=\"\"\"Base EMA\n",
        "        parameter for teacher update. The value is increased to 1 during training with cosine schedule.\n",
        "        We recommend setting a higher value with small batches: for example use 0.9995 with batch size of 256.\"\"\")\n",
        "    parser.add_argument('--use_bn_in_head', default=False, type=utils.bool_flag,\n",
        "        help=\"Whether to use batch normalizations in projection head (Default: False)\")\n",
        "\n",
        "    # Temperature teacher parameters\n",
        "    parser.add_argument('--warmup_teacher_temp', default=0.04, type=float,\n",
        "        help=\"\"\"Initial value for the teacher temperature: 0.04 works well in most cases.\n",
        "        Try decreasing it if the training loss does not decrease.\"\"\")\n",
        "    parser.add_argument('--teacher_temp', default=0.04, type=float, help=\"\"\"Final value (after linear warmup)\n",
        "        of the teacher temperature. For most experiments, anything above 0.07 is unstable. We recommend\n",
        "        starting with the default value of 0.04 and increase this slightly if needed.\"\"\")\n",
        "    parser.add_argument('--warmup_teacher_temp_epochs', default=0, type=int,\n",
        "        help='Number of warmup epochs for the teacher temperature (Default: 30).')\n",
        "\n",
        "    # Training/Optimization parameters\n",
        "    parser.add_argument('--use_fp16', type=utils.bool_flag, default=True, help=\"\"\"Whether or not\n",
        "        to use half precision for training. Improves training time and memory requirements,\n",
        "        but can provoke instability and slight decay of performance. We recommend disabling\n",
        "        mixed precision if the loss is unstable, if reducing the patch size or if training with bigger ViTs.\"\"\")\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.04, help=\"\"\"Initial value of the\n",
        "        weight decay. With ViT, a smaller value at the beginning of training works well.\"\"\")\n",
        "    parser.add_argument('--weight_decay_end', type=float, default=0.4, help=\"\"\"Final value of the\n",
        "        weight decay. We use a cosine schedule for WD and using a larger decay by\n",
        "        the end of training improves performance for ViTs.\"\"\")\n",
        "    parser.add_argument('--clip_grad', type=float, default=3.0, help=\"\"\"Maximal parameter\n",
        "        gradient norm if using gradient clipping. Clipping with norm .3 ~ 1.0 can\n",
        "        help optimization for larger ViT architectures. 0 for disabling.\"\"\")\n",
        "    parser.add_argument('--batch_size_per_gpu', default=32, type=int,\n",
        "        help='Per-GPU batch-size : number of distinct images loaded on one GPU.')\n",
        "    parser.add_argument('--epochs', default=10, type=int, help='Number of epochs of training.')\n",
        "    parser.add_argument('--freeze_last_layer', default=1, type=int, help=\"\"\"Number of epochs\n",
        "        during which we keep the output layer fixed. Typically doing so during\n",
        "        the first epoch helps training. Try increasing this value if the loss does not decrease.\"\"\")\n",
        "    parser.add_argument(\"--lr\", default=0.0005, type=float, help=\"\"\"Learning rate at the end of\n",
        "        linear warmup (highest LR used during training). The learning rate is linearly scaled\n",
        "        with the batch size, and specified here for a reference batch size of 256.\"\"\")\n",
        "    parser.add_argument(\"--warmup_epochs\", default=10, type=int,\n",
        "        help=\"Number of epochs for the linear learning-rate warm up.\")\n",
        "    parser.add_argument('--min_lr', type=float, default=1e-6, help=\"\"\"Target LR at the\n",
        "        end of optimization. We use a cosine LR schedule with linear warmup.\"\"\")\n",
        "    parser.add_argument('--optimizer', default='adamw', type=str,\n",
        "        choices=['adamw', 'sgd', 'lars'], help=\"\"\"Type of optimizer. We recommend using adamw with ViTs.\"\"\")\n",
        "    parser.add_argument('--drop_path_rate', type=float, default=0.1, help=\"stochastic depth rate\")\n",
        "\n",
        "    # Multi-crop parameters\n",
        "    parser.add_argument('--global_crops_scale', type=float, nargs='+', default=(0.4, 1.),\n",
        "        help=\"\"\"Scale range of the cropped image before resizing, relatively to the origin image.\n",
        "        Used for large global view cropping. When disabling multi-crop (--local_crops_number 0), we\n",
        "        recommand using a wider range of scale (\"--global_crops_scale 0.14 1.\" for example)\"\"\")\n",
        "    parser.add_argument('--local_crops_number', type=int, default=8, help=\"\"\"Number of small\n",
        "        local views to generate. Set this parameter to 0 to disable multi-crop training.\n",
        "        When disabling multi-crop we recommend to use \"--global_crops_scale 0.14 1.\" \"\"\")\n",
        "    parser.add_argument('--local_crops_scale', type=float, nargs='+', default=(0.05, 0.4),\n",
        "        help=\"\"\"Scale range of the cropped image before resizing, relatively to the origin image.\n",
        "        Used for small local view cropping of multi-crop.\"\"\")\n",
        "\n",
        "    # Misc\n",
        "    parser.add_argument('--data_path', default='/path/to/imagenet/train/', type=str,\n",
        "        help='Please specify path to the ImageNet training data.')\n",
        "    parser.add_argument('--output_dir', default=\"/content/drive/MyDrive/colab_checkpoints/\", type=str, help='Path to save logs and checkpoints.')\n",
        "    parser.add_argument('--saveckp_freq', default=1, type=int, help='Save checkpoint every x epochs.')\n",
        "    parser.add_argument('--seed', default=0, type=int, help='Random seed.')\n",
        "    parser.add_argument('--num_workers', default=10, type=int, help='Number of data loading workers per GPU.')\n",
        "    parser.add_argument(\"--dist_url\", default=\"env://\", type=str, help=\"\"\"url used to set up\n",
        "        distributed training; see https://pytorch.org/docs/stable/distributed.html\"\"\")\n",
        "    parser.add_argument(\"--local_rank\", default=0, type=int, help=\"Please ignore and do not set this argument.\")\n",
        "    parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "    return parser\n"
      ],
      "metadata": {
        "id": "ZY6W0878UvYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('DINO', parents=[get_args_parser()])\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii3KpbWAU39V",
        "outputId": "cd069239-ceb1-4a41-b17e-d1bce7571833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_xcit_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRpiQ2H8dOQe",
        "outputId": "973b3664-4098-4c43-817a-e6cf22b738d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(arch='vit_tiny', patch_size=16, out_dim=65536, norm_last_layer=True, momentum_teacher=0.996, use_bn_in_head=False, warmup_teacher_temp=0.04, teacher_temp=0.04, warmup_teacher_temp_epochs=0, use_fp16=True, weight_decay=0.04, weight_decay_end=0.4, clip_grad=3.0, batch_size_per_gpu=32, epochs=10, freeze_last_layer=1, lr=0.0005, warmup_epochs=10, min_lr=1e-06, optimizer='adamw', drop_path_rate=0.1, global_crops_scale=(0.4, 1.0), local_crops_number=8, local_crops_scale=(0.05, 0.4), data_path='/path/to/imagenet/train/', output_dir='/content/drive/MyDrive/colab_checkpoints/', saveckp_freq=1, seed=0, num_workers=10, dist_url='env://', local_rank=0, file='/root/.local/share/jupyter/runtime/kernel-48565ec0-cf7d-4ef7-8559-49cd2748b44b.json')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dino(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LETUDz70d4pC",
        "outputId": "23c90973-b7fe-4591-900b-d607f37a0d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git:\n",
            "  sha: N/A, status: clean, branch: N/A\n",
            "\n",
            "arch: vit_tiny\n",
            "batch_size_per_gpu: 32\n",
            "clip_grad: 3.0\n",
            "data_path: /path/to/imagenet/train/\n",
            "dist_url: env://\n",
            "drop_path_rate: 0.1\n",
            "epochs: 10\n",
            "file: /root/.local/share/jupyter/runtime/kernel-48565ec0-cf7d-4ef7-8559-49cd2748b44b.json\n",
            "freeze_last_layer: 1\n",
            "global_crops_scale: (0.4, 1.0)\n",
            "gpu: 0\n",
            "local_crops_number: 8\n",
            "local_crops_scale: (0.05, 0.4)\n",
            "local_rank: 0\n",
            "lr: 0.0005\n",
            "min_lr: 1e-06\n",
            "momentum_teacher: 0.996\n",
            "norm_last_layer: True\n",
            "num_workers: 10\n",
            "optimizer: adamw\n",
            "out_dim: 65536\n",
            "output_dir: /content/drive/MyDrive/colab_checkpoints/\n",
            "patch_size: 16\n",
            "saveckp_freq: 1\n",
            "seed: 0\n",
            "teacher_temp: 0.04\n",
            "use_bn_in_head: False\n",
            "use_fp16: True\n",
            "warmup_epochs: 10\n",
            "warmup_teacher_temp: 0.04\n",
            "warmup_teacher_temp_epochs: 0\n",
            "weight_decay: 0.04\n",
            "weight_decay_end: 0.4\n",
            "building dataset...\n",
            "(50000, 3072)\n",
            "(50000, 32, 32, 3)\n",
            "Data loaded: there are 50000 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student and Teacher are built: they are both vit_tiny network.\n",
            "Loss, optimizer and schedulers ready.\n",
            "Found checkpoint at /content/drive/MyDrive/colab_checkpoints/checkpoint.pth\n",
            "=> loaded 'student' from checkpoint '/content/drive/MyDrive/colab_checkpoints/checkpoint.pth' with msg <All keys matched successfully>\n",
            "=> loaded 'teacher' from checkpoint '/content/drive/MyDrive/colab_checkpoints/checkpoint.pth' with msg <All keys matched successfully>\n",
            "=> loaded 'optimizer' from checkpoint: '/content/drive/MyDrive/colab_checkpoints/checkpoint.pth'\n",
            "=> loaded 'fp16_scaler' from checkpoint: '/content/drive/MyDrive/colab_checkpoints/checkpoint.pth'\n",
            "=> loaded 'dino_loss' from checkpoint '/content/drive/MyDrive/colab_checkpoints/checkpoint.pth' with msg <All keys matched successfully>\n",
            "Starting DINO training !\n",
            "Epoch: [1/10]  [   0/1562]  eta: 5:31:32  loss: 9.274915 (9.274915)  lr: 0.000006 (0.000006)  wd: 0.048810 (0.048810)  time: 12.735477  data: 8.613511  max mem: 12184\n",
            "Epoch: [1/10]  [  10/1562]  eta: 0:38:12  loss: 9.274915 (9.133667)  lr: 0.000006 (0.000006)  wd: 0.048866 (0.048866)  time: 1.477316  data: 0.783554  max mem: 12425\n",
            "Epoch: [1/10]  [  20/1562]  eta: 0:24:34  loss: 8.937002 (9.105681)  lr: 0.000006 (0.000006)  wd: 0.048922 (0.048922)  time: 0.366976  data: 0.005804  max mem: 12425\n",
            "Epoch: [1/10]  [  30/1562]  eta: 0:19:49  loss: 9.085712 (9.105096)  lr: 0.000006 (0.000006)  wd: 0.049035 (0.048979)  time: 0.391379  data: 0.008283  max mem: 12425\n",
            "Epoch: [1/10]  [  40/1562]  eta: 0:17:18  loss: 9.094114 (9.096413)  lr: 0.000006 (0.000006)  wd: 0.049149 (0.049035)  time: 0.395296  data: 0.005303  max mem: 12425\n",
            "Epoch: [1/10]  [  50/1562]  eta: 0:15:48  loss: 8.997159 (9.066999)  lr: 0.000006 (0.000006)  wd: 0.049263 (0.049092)  time: 0.395101  data: 0.003271  max mem: 12425\n",
            "Epoch: [1/10]  [  60/1562]  eta: 0:14:45  loss: 8.922154 (9.070904)  lr: 0.000006 (0.000006)  wd: 0.049378 (0.049150)  time: 0.399386  data: 0.002792  max mem: 12425\n",
            "Epoch: [1/10]  [  70/1562]  eta: 0:13:57  loss: 8.818381 (9.029168)  lr: 0.000006 (0.000006)  wd: 0.049494 (0.049207)  time: 0.393204  data: 0.003221  max mem: 12425\n",
            "Epoch: [1/10]  [  80/1562]  eta: 0:13:25  loss: 8.810034 (8.994595)  lr: 0.000007 (0.000006)  wd: 0.049610 (0.049265)  time: 0.401903  data: 0.004971  max mem: 12425\n",
            "Epoch: [1/10]  [  90/1562]  eta: 0:12:56  loss: 8.790528 (8.980405)  lr: 0.000007 (0.000006)  wd: 0.049727 (0.049323)  time: 0.406947  data: 0.005629  max mem: 12425\n",
            "Epoch: [1/10]  [ 100/1562]  eta: 0:12:28  loss: 8.804242 (8.969325)  lr: 0.000007 (0.000006)  wd: 0.049845 (0.049381)  time: 0.384145  data: 0.003287  max mem: 12425\n",
            "Epoch: [1/10]  [ 110/1562]  eta: 0:12:10  loss: 8.947469 (8.970191)  lr: 0.000007 (0.000006)  wd: 0.049963 (0.049439)  time: 0.393221  data: 0.004046  max mem: 12425\n",
            "Epoch: [1/10]  [ 120/1562]  eta: 0:11:53  loss: 9.030011 (8.977829)  lr: 0.000007 (0.000006)  wd: 0.050082 (0.049498)  time: 0.407813  data: 0.004439  max mem: 12425\n",
            "Epoch: [1/10]  [ 130/1562]  eta: 0:11:41  loss: 8.938643 (8.963339)  lr: 0.000007 (0.000007)  wd: 0.050202 (0.049557)  time: 0.417255  data: 0.029349  max mem: 12425\n",
            "Epoch: [1/10]  [ 140/1562]  eta: 0:11:32  loss: 8.897515 (8.951349)  lr: 0.000007 (0.000007)  wd: 0.050323 (0.049616)  time: 0.443451  data: 0.057247  max mem: 12425\n",
            "Epoch: [1/10]  [ 150/1562]  eta: 0:11:21  loss: 8.792975 (8.941772)  lr: 0.000007 (0.000007)  wd: 0.050444 (0.049675)  time: 0.435055  data: 0.040348  max mem: 12425\n",
            "Epoch: [1/10]  [ 160/1562]  eta: 0:11:14  loss: 8.873333 (8.945914)  lr: 0.000007 (0.000007)  wd: 0.050566 (0.049734)  time: 0.436268  data: 0.037656  max mem: 12425\n",
            "Epoch: [1/10]  [ 170/1562]  eta: 0:11:10  loss: 8.851012 (8.943195)  lr: 0.000007 (0.000007)  wd: 0.050688 (0.049794)  time: 0.473479  data: 0.084862  max mem: 12425\n",
            "Epoch: [1/10]  [ 180/1562]  eta: 0:10:59  loss: 8.851012 (8.942877)  lr: 0.000007 (0.000007)  wd: 0.050811 (0.049854)  time: 0.448403  data: 0.065329  max mem: 12425\n",
            "Epoch: [1/10]  [ 190/1562]  eta: 0:10:50  loss: 8.922558 (8.950152)  lr: 0.000007 (0.000007)  wd: 0.050935 (0.049914)  time: 0.407651  data: 0.009340  max mem: 12425\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b357c127c9a2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dino\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-3191fda7910d>\u001b[0m in \u001b[0;36mtrain_dino\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# ============ training one epoch of DINO ... ============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         train_stats = train_one_epoch(student, teacher, teacher_without_ddp, dino_loss,\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_schedule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             epoch, fp16_scaler, args)\n",
            "\u001b[0;32m<ipython-input-22-fb3f4b03066a>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(student, teacher, teacher_without_ddp, dino_loss, data_loader, optimizer, lr_schedule, wd_schedule, momentum_schedule, epoch, fp16_scaler, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mfp16_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unscale the gradients of optimizer's assigned params in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mparam_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             utils.cancel_gradients_last_layer(epoch, student,\n\u001b[1;32m     42\u001b[0m                                               args.freeze_last_layer)\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/utils.py\u001b[0m in \u001b[0;36mclip_gradients\u001b[0;34m(model, clip)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}